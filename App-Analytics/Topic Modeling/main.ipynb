{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we first load the data into a pandas DataFrame. Then, we define a function for preprocessing the text data, which removes stop words and unnecessary characters. Next, we tokenize the text data using the simple_preprocess function from gensim. After that, we create a dictionary and a document-term matrix using the corpora module from gensim. Finally, we apply the LDA algorithm on the document-term matrix to identify the topics in the reviews. We print the top 10 topics and their corresponding words. You can adjust the number of topics by changing the num_topics parameter in the LdaModel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a pandas DataFrame\n",
    "data = pd.read_csv('../../Warehouse/Reviews/app_reviews_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Iteration:  2\n",
      "Iteration:  3\n",
      "Iteration:  4\n",
      "Iteration:  5\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "\n",
    "    data = pd.read_csv('../../Warehouse/Reviews/app_reviews_'+str(i)+'.csv')\n",
    "\n",
    "    print('Iteration: ', i)\n",
    "\n",
    "    # Tokenization\n",
    "    tokenized_data = data['content'].apply(preprocess)\n",
    "\n",
    "    # Vectorization\n",
    "    dictionary = corpora.Dictionary(tokenized_data)\n",
    "    doc_term_matrix = [dictionary.doc2bow(tokens) for tokens in tokenized_data]\n",
    "\n",
    "    # Topic Modeling\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=10, id2word=dictionary, passes=50)\n",
    "    \n",
    "    final_model = lda_model\n",
    "    model_name = 'lda_model_net_data_10_topics_' + str(i)\n",
    "\n",
    "    # Saving the model in a pickle file\n",
    "    with open(model_name + '.pkl', 'wb') as file:\n",
    "        pickle.dump(final_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk with 1 review as loaded_model\n",
    "loaded_model = pickle.load(open('lda_model_net_data_10_topics_1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Problem & Issues: \n",
      "Words: 0.161*\"good\" + 0.070*\"application\" + 0.057*\"fake\" + 0.031*\"time\" + 0.029*\"wrost\" + 0.025*\"long\" + 0.024*\"location\" + 0.021*\"device\" + 0.021*\"waiting\" + 0.018*\"register\"\n",
      "Topic Usability & Experience: \n",
      "Words: 0.077*\"server\" + 0.041*\"scan\" + 0.033*\"issues\" + 0.032*\"nahi\" + 0.025*\"mere\" + 0.022*\"bekar\" + 0.022*\"free\" + 0.020*\"paise\" + 0.014*\"sign\" + 0.013*\"technical\"\n",
      "Topic Installation & Time: \n",
      "Words: 0.201*\"worst\" + 0.078*\"download\" + 0.057*\"work\" + 0.037*\"dont\" + 0.036*\"experience\" + 0.033*\"install\" + 0.028*\"time\" + 0.028*\"registration\" + 0.023*\"seen\" + 0.019*\"slow\"\n",
      "Topic Cards & Data: \n",
      "Words: 0.179*\"problem\" + 0.064*\"issue\" + 0.064*\"solve\" + 0.034*\"login\" + 0.025*\"fampay\" + 0.023*\"problems\" + 0.022*\"soon\" + 0.019*\"facing\" + 0.019*\"able\" + 0.018*\"possible\"\n",
      "Topic Functionality & Payments: \n",
      "Words: 0.088*\"working\" + 0.065*\"fampay\" + 0.037*\"account\" + 0.026*\"open\" + 0.021*\"help\" + 0.019*\"days\" + 0.019*\"showing\" + 0.018*\"months\" + 0.016*\"error\" + 0.014*\"shows\"\n",
      "Topic Features & Design: \n",
      "Words: 0.169*\"payment\" + 0.057*\"time\" + 0.052*\"transaction\" + 0.041*\"useless\" + 0.029*\"payments\" + 0.029*\"processing\" + 0.029*\"poor\" + 0.028*\"worst\" + 0.025*\"stuck\" + 0.022*\"failed\"\n",
      "Topic Security & Privacy: \n",
      "Words: 0.091*\"service\" + 0.070*\"customer\" + 0.052*\"available\" + 0.039*\"care\" + 0.038*\"support\" + 0.034*\"code\" + 0.032*\"reply\" + 0.022*\"order\" + 0.020*\"services\" + 0.018*\"response\"\n",
      "Topic Support & Service: \n",
      "Words: 0.185*\"money\" + 0.067*\"account\" + 0.054*\"fampay\" + 0.029*\"send\" + 0.018*\"rupees\" + 0.018*\"bank\" + 0.017*\"received\" + 0.016*\"refund\" + 0.016*\"friend\" + 0.013*\"transfer\"\n",
      "Topic Updates & Performance: \n",
      "Words: 0.240*\"card\" + 0.063*\"nice\" + 0.030*\"details\" + 0.029*\"fampay\" + 0.026*\"famcard\" + 0.019*\"raha\" + 0.019*\"best\" + 0.017*\"debit\" + 0.016*\"nahi\" + 0.016*\"mera\"\n",
      "Topic Notifications & Ads: \n",
      "Words: 0.058*\"waste\" + 0.055*\"option\" + 0.039*\"number\" + 0.037*\"star\" + 0.028*\"parents\" + 0.025*\"time\" + 0.024*\"reward\" + 0.022*\"want\" + 0.020*\"change\" + 0.020*\"aadhar\"\n"
     ]
    }
   ],
   "source": [
    "topic_labels = {\n",
    "    0: \"Problem & Issues\",\n",
    "    1: \"Usability & Experience\",\n",
    "    2: \"Installation & Time\",\n",
    "    3: \"Cards & Data\",\n",
    "    4: \"Functionality & Payments\",\n",
    "    5: \"Features & Design\",\n",
    "    6: \"Security & Privacy\",\n",
    "    7: \"Support & Service\",\n",
    "    8: \"Updates & Performance\",\n",
    "    9: \"Notifications & Ads\",\n",
    "    10: \"Other\"\n",
    "}\n",
    "\n",
    "# Print the topics with meaningful labels\n",
    "for idx, topic in loaded_model.print_topics(-1):\n",
    "    print(f\"Topic {topic_labels.get(idx, idx)}: \\nWords: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Problem & Issues mainly discusses good, application, fake, time, and wrost.\n",
      "\n",
      "Topic Usability & Experience mainly discusses server, scan, issues, nahi, and mere.\n",
      "\n",
      "Topic Installation & Time mainly discusses worst, download, work, dont, and experience.\n",
      "\n",
      "Topic Cards & Data mainly discusses problem, issue, solve, login, and fampay.\n",
      "\n",
      "Topic Functionality & Payments mainly discusses working, fampay, account, open, and help.\n",
      "\n",
      "Topic Features & Design mainly discusses payment, time, transaction, useless, and payments.\n",
      "\n",
      "Topic Security & Privacy mainly discusses service, customer, available, care, and support.\n",
      "\n",
      "Topic Support & Service mainly discusses money, account, fampay, send, and rupees.\n",
      "\n",
      "Topic Updates & Performance mainly discusses card, nice, details, fampay, and famcard.\n",
      "\n",
      "Topic Notifications & Ads mainly discusses waste, option, number, star, and parents.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = \"\"\n",
    "for idx, topic in loaded_model.print_topics(-1):\n",
    "    topic_name = topic_labels.get(idx, idx)\n",
    "    top_words = [word_prob.split('*')[1].strip('\"') for word_prob in topic.split(' + ')[:5]]\n",
    "    summary += f\"Topic {topic_name} mainly discusses {', '.join(top_words[:-1])}, and {top_words[-1]}.\\n\\n\"\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
