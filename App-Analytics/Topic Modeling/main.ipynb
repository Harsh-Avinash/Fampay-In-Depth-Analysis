{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we first load the data into a pandas DataFrame. Then, we define a function for preprocessing the text data, which removes stop words and unnecessary characters. Next, we tokenize the text data using the simple_preprocess function from gensim. After that, we create a dictionary and a document-term matrix using the corpora module from gensim. Finally, we apply the LDA algorithm on the document-term matrix to identify the topics in the reviews. We print the top 10 topics and their corresponding words. You can adjust the number of topics by changing the num_topics parameter in the LdaModel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a pandas DataFrame\n",
    "data = pd.read_csv('../../Warehouse/Reviews/app_reviews_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Iteration:  2\n",
      "Iteration:  3\n",
      "Iteration:  4\n",
      "Iteration:  5\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "\n",
    "    data = pd.read_csv('../../Warehouse/Reviews/app_reviews_'+str(i)+'.csv')\n",
    "\n",
    "    print('Iteration: ', i)\n",
    "\n",
    "    # Tokenization\n",
    "    tokenized_data = data['content'].apply(preprocess)\n",
    "\n",
    "    # Vectorization\n",
    "    dictionary = corpora.Dictionary(tokenized_data)\n",
    "    doc_term_matrix = [dictionary.doc2bow(tokens) for tokens in tokenized_data]\n",
    "\n",
    "    # Topic Modeling\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=10, id2word=dictionary, passes=50)\n",
    "    \n",
    "    final_model = lda_model\n",
    "    model_name = 'lda_model_net_data_10_topics_' + str(i)\n",
    "\n",
    "    # Saving the model in a pickle file\n",
    "    with open(model_name + '.pkl', 'wb') as file:\n",
    "        pickle.dump(final_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with the model for review score:  4\n"
     ]
    }
   ],
   "source": [
    "review_score = int(input('Enter the review score: '))\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open('lda_model_net_data_10_topics_'+str(review_score)+'.pkl', 'rb'))\n",
    "print(\"Working with the model for review score: \", review_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Problem & Issues: \n",
      "Words: 0.115*\"great\" + 0.057*\"service\" + 0.041*\"code\" + 0.032*\"scan\" + 0.027*\"need\" + 0.026*\"option\" + 0.025*\"customer\" + 0.024*\"transactions\" + 0.022*\"improve\" + 0.020*\"wonderful\"\n",
      "Topic Usability & Experience: \n",
      "Words: 0.141*\"awesome\" + 0.118*\"easy\" + 0.039*\"children\" + 0.034*\"reward\" + 0.024*\"banking\" + 0.023*\"registration\" + 0.022*\"options\" + 0.020*\"perfect\" + 0.019*\"thing\" + 0.015*\"offers\"\n",
      "Topic Installation & Time: \n",
      "Words: 0.593*\"nice\" + 0.030*\"excellent\" + 0.026*\"better\" + 0.022*\"teenagers\" + 0.006*\"teens\" + 0.006*\"platform\" + 0.006*\"work\" + 0.005*\"withdrawal\" + 0.004*\"facility\" + 0.004*\"supper\"\n",
      "Topic Cards & Data: \n",
      "Words: 0.111*\"super\" + 0.055*\"number\" + 0.049*\"mobile\" + 0.041*\"change\" + 0.038*\"experience\" + 0.032*\"mast\" + 0.031*\"teen\" + 0.023*\"works\" + 0.016*\"fine\" + 0.014*\"earning\"\n",
      "Topic Functionality & Payments: \n",
      "Words: 0.136*\"card\" + 0.090*\"fampay\" + 0.036*\"love\" + 0.030*\"help\" + 0.027*\"available\" + 0.024*\"famcard\" + 0.023*\"thanks\" + 0.019*\"happy\" + 0.016*\"start\" + 0.016*\"want\"\n",
      "Topic Features & Design: \n",
      "Words: 0.179*\"best\" + 0.102*\"teenagers\" + 0.101*\"payment\" + 0.049*\"useful\" + 0.041*\"application\" + 0.031*\"helpful\" + 0.022*\"teenager\" + 0.022*\"teens\" + 0.022*\"slow\" + 0.020*\"online\"\n",
      "Topic Security & Privacy: \n",
      "Words: 0.643*\"good\" + 0.033*\"teenagers\" + 0.015*\"experience\" + 0.011*\"overall\" + 0.010*\"application\" + 0.009*\"teens\" + 0.007*\"fantastic\" + 0.007*\"teenager\" + 0.007*\"kids\" + 0.006*\"location\"\n",
      "Topic Support & Service: \n",
      "Words: 0.102*\"money\" + 0.068*\"like\" + 0.035*\"bank\" + 0.028*\"account\" + 0.027*\"transfer\" + 0.020*\"phone\" + 0.020*\"send\" + 0.016*\"online\" + 0.015*\"paytm\" + 0.014*\"fampay\"\n",
      "Topic Updates & Performance: \n",
      "Words: 0.086*\"amazing\" + 0.076*\"payments\" + 0.059*\"working\" + 0.025*\"apps\" + 0.024*\"fast\" + 0.022*\"scanner\" + 0.017*\"safe\" + 0.015*\"work\" + 0.014*\"transaction\" + 0.014*\"secure\"\n",
      "Topic Notifications & Ads: \n",
      "Words: 0.071*\"problem\" + 0.035*\"fampay\" + 0.034*\"time\" + 0.027*\"payment\" + 0.024*\"issue\" + 0.023*\"money\" + 0.020*\"solve\" + 0.017*\"update\" + 0.015*\"server\" + 0.015*\"star\"\n"
     ]
    }
   ],
   "source": [
    "topic_labels = {\n",
    "    0: \"Problem & Issues\",\n",
    "    1: \"Usability & Experience\",\n",
    "    2: \"Installation & Time\",\n",
    "    3: \"Cards & Data\",\n",
    "    4: \"Functionality & Payments\",\n",
    "    5: \"Features & Design\",\n",
    "    6: \"Security & Privacy\",\n",
    "    7: \"Support & Service\",\n",
    "    8: \"Updates & Performance\",\n",
    "    9: \"Notifications & Ads\",\n",
    "    10: \"Other\"\n",
    "}\n",
    "\n",
    "# Print the topics with meaningful labels\n",
    "for idx, topic in loaded_model.print_topics(-1):\n",
    "    print(f\"Topic {topic_labels.get(idx, idx)}: \\nWords: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_term_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Visualize the topics\u001b[39;00m\n\u001b[0;32m      2\u001b[0m lda_model \u001b[39m=\u001b[39m loaded_model\n\u001b[1;32m----> 3\u001b[0m lda_display \u001b[39m=\u001b[39m pyLDAvis\u001b[39m.\u001b[39mgensim_models\u001b[39m.\u001b[39mprepare(lda_model, doc_term_matrix, dictionary)\n\u001b[0;32m      4\u001b[0m pyLDAvis\u001b[39m.\u001b[39mdisplay(lda_display)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc_term_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "lda_model = loaded_model\n",
    "lda_display = pyLDAvis.gensim_models.prepare(lda_model, doc_term_matrix, dictionary)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Problem & Issues mainly discusses recommend, digital, money, safe, and interface.\n",
      "\n",
      "Topic Usability & Experience mainly discusses card, debit, payments, teenager, and helpful.\n",
      "\n",
      "Topic Installation & Time mainly discusses teenagers, money, transfer, payment, and bank.\n",
      "\n",
      "Topic Cards & Data mainly discusses good, happy, wonderful, fantastic, and application.\n",
      "\n",
      "Topic Functionality & Payments mainly discusses easy, payment, online, payments, and fast.\n",
      "\n",
      "Topic Features & Design mainly discusses nice, mast, application, lovely, and appp.\n",
      "\n",
      "Topic Security & Privacy mainly discusses best, teens, excellent, teen, and kids.\n",
      "\n",
      "Topic Support & Service mainly discusses great, amazing, awesome, like, and useful.\n",
      "\n",
      "Topic Updates & Performance mainly discusses love, fampay, transactions, thank, and download.\n",
      "\n",
      "Topic Notifications & Ads mainly discusses fampay, super, problem, team, and help.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = \"\"\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    topic_name = topic_labels.get(idx, idx)\n",
    "    top_words = [word_prob.split('*')[1].strip('\"') for word_prob in topic.split(' + ')[:5]]\n",
    "    summary += f\"Topic {topic_name} mainly discusses {', '.join(top_words[:-1])}, and {top_words[-1]}.\\n\\n\"\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
